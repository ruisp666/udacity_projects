{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tennis: AI Playing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here, we load the trained agents, and let the agents play some games."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from unityagents import UnityEnvironment\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:unityagents:\n",
      "'Academy' started successfully!\n",
      "Unity Academy name: Academy\n",
      "        Number of Brains: 1\n",
      "        Number of External Brains : 1\n",
      "        Lesson number : 0\n",
      "        Reset Parameters :\n",
      "\t\t\n",
      "Unity brain name: TennisBrain\n",
      "        Number of Visual Observations (per agent): 0\n",
      "        Vector Observation space type: continuous\n",
      "        Vector Observation space size (per agent): 8\n",
      "        Number of stacked Vector Observation: 3\n",
      "        Vector Action space type: continuous\n",
      "        Vector Action space size (per agent): 2\n",
      "        Vector Action descriptions: , \n"
     ]
    }
   ],
   "source": [
    "# Load the environent\n",
    "env = UnityEnvironment(file_name=\"Tennis.app\")\n",
    "\n",
    "# Get the default brain\n",
    "brain_name = env.brain_names[0]\n",
    "brain = env.brains[brain_name]\n",
    "\n",
    "# Reset the environment, this time train_mode is false\n",
    "env_info = env.reset(train_mode=False)[brain_name]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of agents: 2\n",
      "Size of each action: 2\n",
      "Size of each state: 24\n"
     ]
    }
   ],
   "source": [
    "# Number of agents \n",
    "num_agents = len(env_info.agents)\n",
    "print('Number of agents:', num_agents)\n",
    "\n",
    "# Size of each action\n",
    "action_size = brain.vector_action_space_size\n",
    "print('Size of each action:', action_size)\n",
    "\n",
    "# Examine the state space \n",
    "states = env_info.vector_observations\n",
    "state_size = states.shape[1]\n",
    "print('Size of each state:', state_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We load the agent class and the the saved weights into the multiagent instance."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now load the weights.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from multi_afrom multigent_ddpg import MultiAgent\n",
    "multiagent = MultiAgent(num_agents, state_size, action_size, seed=2345)\n",
    "\n",
    "# Load the weights\n",
    "for i, agent in enumerate(multiagent.agents):\n",
    "    agent.actor_local.load_state_dict(torch.load(f'checkpoint_actor_{i}.pth'))\n",
    "    agent.critic_local.load_state_dict(torch.load(f'checkpoint_critic{i}.pth'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us play 10 games, and see the individual and aggregate performance. Recall that our trained multiagent obtained an average score over 100 epsiodes of 1.29."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Score (max over agents) from episode 1: 2.1000000312924385\n",
      "Score (max over agents) from episode 2: 0.10000000149011612\n",
      "Score (max over agents) from episode 3: 2.600000038743019\n",
      "Score (max over agents) from episode 4: 0.7000000104308128\n",
      "Score (max over agents) from episode 5: 1.8000000268220901\n",
      "Score (max over agents) from episode 6: 0.4000000059604645\n",
      "Score (max over agents) from episode 7: 1.0000000149011612\n",
      "Score (max over agents) from episode 8: 0.0\n",
      "Score (max over agents) from episode 9: 2.600000038743019\n",
      "Score (max over agents) from episode 10: 1.1000000163912773\n",
      "Average maximum over all the epsiodes: 1.2400000184774398\n"
     ]
    }
   ],
   "source": [
    "n_ep = 10\n",
    "scores_ep = []\n",
    "for i in range(1, n_ep + 1):                                      # play game for 5 episodes\n",
    "    env_info = env.reset(train_mode=False)[brain_name]     # reset the environment    \n",
    "    states = env_info.vector_observations                  # get the current state (for each agent)\n",
    "    scores = np.zeros(num_agents)                          # initialize the score (for each agent)\n",
    "    while True:\n",
    "        actions = multiagent.act(states, 0) # Noise is zero\n",
    "        env_info = env.step(actions)[brain_name]           # send all actions to tne environment\n",
    "        next_states = env_info.vector_observations         # get next state (for each agent)\n",
    "        rewards = env_info.rewards                         # get reward (for each agent)\n",
    "        dones = env_info.local_done                        # see if episode finished\n",
    "        scores += env_info.rewards                         # update the score (for each agent)\n",
    "        states = next_states                               # roll over states to next time step\n",
    "        if np.any(dones):                                  # exit loop if episode finished\n",
    "            break\n",
    "    print('Score (max over agents) from episode {}: {}'.format(i, np.max(scores)))\n",
    "    scores_ep.append(np.max(scores))\n",
    "print(f'Average maximum over all the epsiodes: {np.mean(scores_ep)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see that in 10 games, the distribution of the scores is very unstable, but the average is still within the expected range. Finally, we close the environment, so that we avoid leaving it hanging. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "env.close()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "deep rl (torch)",
   "language": "python",
   "name": "drlnd"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
